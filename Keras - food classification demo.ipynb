{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Demo - Food Photo Classification\n",
    "\n",
    "In this demo, we are going to build a convolutional neural network using Keras, and train it to classify 4 types of foods - Ais Kacang, Ang Ku Kueh, Apam Balik, and Asam Laksa.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"datasets/train/AisKacang/001.jpg\" width=200 height=200 /></td>\n",
    "<td><img src=\"datasets/train/AngKuKueh/050.jpg\" width=200 height=200 /></td>\n",
    "<td><img src=\"datasets/train/ApamBalik/015.jpg\" width=200 height=200 /></td>\n",
    "<td><img src=\"datasets/train/AsamLaksa/012.jpg\" width=200 height=200 /></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The dataset photos are provided by [Jack](https://github.com/jackg0h/) from the FoodTag project. Try send a photo to the [Messenger bot](https://www.messenger.com/t/686970151472860) - it can recognize 100 types of Malaysian foods!\n",
    "\n",
    "**!** The full dataset will not be made available on GitHub. You can however, try to put in other dataset like [Dogs & Cats](https://www.kaggle.com/c/dogs-vs-cats).\n",
    "\n",
    "References (citation mesti bagi):\n",
    "1. [Where I copied & pasted sample codes from](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n",
    "2. Goh Cheng Kee, Wong Chin Yee, John See (2017). FoodTag: Automatic Classification of Food Photos Using Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = 'datasets/train'\n",
    "test_dir  = 'datasets/test'\n",
    "nb_train_samples = 400\n",
    "nb_test_samples = 400\n",
    "img_width = 227\n",
    "img_height = 227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d11b5571-a344-4ecf-ac59-f0108365e052"
    }
   },
   "source": [
    "## Design the neural network\n",
    "\n",
    "Here, we build the convolutional neural network that will be used to classify the images. This is how a basic convnet can look like:\n",
    "\n",
    "![LeNet](img/mylenet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "503aecb6-d4c3-4ac9-ac00-2138cd780668"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Most of these values like nb_filters, nb_rows, nb_columns are mostly trial and error.\n",
    "# There is no definite answer to what is the correct one, just keep trying.\n",
    "# input_shape[0] = 3 because of RGB channels\n",
    "model.add(Convolution2D(32, 5, 5, input_shape=(3, img_width, img_height)))\n",
    "\n",
    "# ReLU ensures none of the output value from CONV layer falls below 0\n",
    "# To prevent vanishing gradient problem (failure to learn) from happening\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# For every 2x2 \"pixels\" in the output feature map, pick the highest value,\n",
    "# more like pick the most activated one.\n",
    "# Most importantly, pooling effectively reduces the feature map size by 75%\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Repeat a few times before going into fully connected (dense) layers\n",
    "# You are free to comment them out or add more conv relu pool layers\n",
    "# See what it does to the accuracy\n",
    "model.add(Convolution2D(64, 5, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(128, 5, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(256, 5, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We can do as many convolutions and poolings as we like, but it's time to really classify\n",
    "# Again, you can do as many dense layers as you like, use different activations and dropout %\n",
    "# It's all trial and error, this demo code may not be the most optimal after all!\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final layer, 4 nodes representing each class, softmax activation makes sure every node sums up to 1\n",
    "# That means the output nodes are in % score for each food.\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# The architecture is there. But how to measure how close we are to perfection?\n",
    "# Measure the difference from prediction to ground truth - the loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "01c045eb-8830-46c3-afe5-272c45577372"
    }
   },
   "source": [
    "## Prepare image data generator\n",
    "\n",
    "The inputs for neural networks are tensors - think matrices of numbers. In case of images, the tensors can be rows and columns of pixels, with each pixel quantised into red, green, blue channels. Hence, the 3 layers of convolutional neural networks, along with its fixed widths and heights.\n",
    "\n",
    "Keras provides this nifty data generator function to generate tensors of images during training. There are plenty of options that can be passed into the generator to augment images in real time. However, we won't be touching those yet.\n",
    "\n",
    "By simply passing in the directory names into the generator, you saved yourself from:\n",
    "* Manually generating RGB matrices from images\n",
    "* Resizing them to reduce processing on neural network's end (full-size photos may not fit into your graphics card VRAM)\n",
    "* Reshuffle them in every epoch\n",
    "\n",
    "See:\n",
    "- [Keras Blog - Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "- [ImageDataGenerator documentation](https://keras.io/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_generator = datagen.flow_from_directory(train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "testing_generator = datagen.flow_from_directory(test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e55b130-fe2d-4866-bf11-4d6c7362e756"
    }
   },
   "source": [
    "## LET'S TRAIN THE CONVNET!\n",
    "\n",
    "Since there are 4 food types (classes) to classify, a neural network that doesn't work will return random results - and yield 0.25 accuracy. So if you network keeps yielding 0.25 `val_acc` after a few epochs, check your setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e29767b5-810b-4237-ac87-a7a86f34e0a8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 14s - loss: 1.4395 - acc: 0.2725 - val_loss: 1.3263 - val_acc: 0.3025\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 12s - loss: 1.2574 - acc: 0.3700 - val_loss: 1.2765 - val_acc: 0.4275\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 13s - loss: 1.2147 - acc: 0.4775 - val_loss: 1.2088 - val_acc: 0.5050\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 12s - loss: 1.1351 - acc: 0.5250 - val_loss: 1.2671 - val_acc: 0.4725\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 12s - loss: 1.1465 - acc: 0.4850 - val_loss: 1.1013 - val_acc: 0.5850\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 12s - loss: 1.0797 - acc: 0.5750 - val_loss: 1.0507 - val_acc: 0.6125\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 12s - loss: 0.9285 - acc: 0.6350 - val_loss: 1.0862 - val_acc: 0.5875\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 12s - loss: 0.9215 - acc: 0.6550 - val_loss: 1.0907 - val_acc: 0.5675\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 12s - loss: 0.8714 - acc: 0.6800 - val_loss: 0.8783 - val_acc: 0.6575\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 12s - loss: 0.7527 - acc: 0.7175 - val_loss: 0.9493 - val_acc: 0.6700\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 12s - loss: 0.6870 - acc: 0.7300 - val_loss: 0.8366 - val_acc: 0.6975\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 12s - loss: 0.6086 - acc: 0.7825 - val_loss: 0.8983 - val_acc: 0.7000\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 12s - loss: 0.5810 - acc: 0.7950 - val_loss: 0.9526 - val_acc: 0.7075\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 12s - loss: 0.4620 - acc: 0.8350 - val_loss: 0.8526 - val_acc: 0.6950\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 12s - loss: 0.3752 - acc: 0.8525 - val_loss: 0.9863 - val_acc: 0.6975\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 12s - loss: 0.3951 - acc: 0.8825 - val_loss: 0.9695 - val_acc: 0.6975\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 12s - loss: 0.2776 - acc: 0.8950 - val_loss: 1.1719 - val_acc: 0.7000\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 12s - loss: 0.2987 - acc: 0.8950 - val_loss: 1.3259 - val_acc: 0.6300\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 12s - loss: 0.3246 - acc: 0.8775 - val_loss: 1.0651 - val_acc: 0.7200\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 13s - loss: 0.1942 - acc: 0.9450 - val_loss: 1.4201 - val_acc: 0.6625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb16eee8a20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        training_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=20,\n",
    "        validation_data=testing_generator,\n",
    "        nb_val_samples=nb_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try augment the image data for higher accuracy\n",
    "\n",
    "In previous image data generator, we simply ask it to stream from the photo folders, and pass the photo at a smaller size into the convnet. We can reach about 70% validation accuracy - that means every 10 times the network is shown a photo they haven't seen before, the network got it right 7 times.\n",
    "\n",
    "A few things we can do to _maybe_ make the training better:\n",
    "\n",
    "* Randomly rotate the images left and right because people don't always take food photos upright?\n",
    "* Randomly flip the photo horizontally?\n",
    "\n",
    "Keras built-in ImageDataGenerator is able to augment these images at training time, this is extremely handy when you have little training data, but you want your model to be able to generalise for wider range of applications - to look at different food photos better.\n",
    "\n",
    "#### How to know if rotating and flipping images is working?\n",
    "Look at the increase of `val_acc` and drop of `val_loss` compared to the log above. That means the neural network is able to generalise for the photos it has never looked at before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 13s - loss: 1.1300 - acc: 0.6400 - val_loss: 0.8480 - val_acc: 0.6800\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 12s - loss: 0.7371 - acc: 0.7275 - val_loss: 0.7639 - val_acc: 0.7275\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 12s - loss: 0.6929 - acc: 0.7275 - val_loss: 0.9595 - val_acc: 0.6600\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 12s - loss: 0.6919 - acc: 0.7400 - val_loss: 0.8661 - val_acc: 0.6750\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 13s - loss: 0.5825 - acc: 0.7725 - val_loss: 0.9389 - val_acc: 0.6800\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 13s - loss: 0.5327 - acc: 0.8025 - val_loss: 0.8647 - val_acc: 0.7375\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 12s - loss: 0.4560 - acc: 0.8475 - val_loss: 0.8999 - val_acc: 0.7150\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 12s - loss: 0.4446 - acc: 0.8475 - val_loss: 0.8371 - val_acc: 0.6875\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 12s - loss: 0.4835 - acc: 0.8325 - val_loss: 0.8565 - val_acc: 0.7425\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 13s - loss: 0.4078 - acc: 0.8625 - val_loss: 0.8461 - val_acc: 0.7550\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 12s - loss: 0.2945 - acc: 0.8950 - val_loss: 0.7835 - val_acc: 0.7775\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 12s - loss: 0.3921 - acc: 0.8600 - val_loss: 0.8739 - val_acc: 0.7525\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 12s - loss: 0.3820 - acc: 0.8500 - val_loss: 0.8088 - val_acc: 0.7050\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 12s - loss: 0.3799 - acc: 0.8550 - val_loss: 0.7529 - val_acc: 0.7675\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 12s - loss: 0.3251 - acc: 0.9050 - val_loss: 0.8347 - val_acc: 0.7425\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 12s - loss: 0.3152 - acc: 0.8775 - val_loss: 0.9507 - val_acc: 0.7575\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 12s - loss: 0.3378 - acc: 0.8750 - val_loss: 0.8036 - val_acc: 0.7750\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 12s - loss: 0.3210 - acc: 0.8950 - val_loss: 1.1079 - val_acc: 0.7500\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 12s - loss: 0.3227 - acc: 0.8975 - val_loss: 0.8933 - val_acc: 0.7575\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 12s - loss: 0.2397 - acc: 0.9200 - val_loss: 0.8071 - val_acc: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb16c66e7b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "training_generator = datagen.flow_from_directory(train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "testing_generator = datagen.flow_from_directory(test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "model.fit_generator(\n",
    "        training_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=20,\n",
    "        validation_data=testing_generator,\n",
    "        nb_val_samples=nb_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It worked!\n",
    "\n",
    "Our dataset is really small, with only 4 types of food and only 100 photos per category to train, `ImageDataGenerator` really is our friend for rotating and flipping our images around so our convolutional neural network can learn to classify foods better - with a peak validating accuracy of 80%!\n",
    "\n",
    "## Explore more Keras\n",
    "\n",
    "1. [Keras homepage](https://keras.io/layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [p3]",
   "language": "python",
   "name": "Python [p3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "8eaa2e2c-fa56-4e39-8dcc-78c13c7628e1": {
     "id": "8eaa2e2c-fa56-4e39-8dcc-78c13c7628e1",
     "layout": "manual",
     "prev": null,
     "regions": {
      "55aee090-d18f-4028-a446-f6d4b8c3e269": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "7bc7d92f-c4ef-4997-bc94-27a1d7774054",
        "part": "source"
       },
       "id": "55aee090-d18f-4028-a446-f6d4b8c3e269"
      },
      "6110c4ae-a98a-4d57-a30b-f68f73c61500": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0.4969429347826087,
        "y": 0.01358695652173913
       },
       "content": {
        "cell": "7bc7d92f-c4ef-4997-bc94-27a1d7774054",
        "part": "outputs"
       },
       "id": "6110c4ae-a98a-4d57-a30b-f68f73c61500"
      }
     }
    }
   },
   "themes": {
    "default": "6b984cc6-d473-4bf7-8a5b-52a343aecfc6",
    "theme": {
     "6b984cc6-d473-4bf7-8a5b-52a343aecfc6": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "6b984cc6-d473-4bf7-8a5b-52a343aecfc6",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         0,
         0,
         0
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         0,
         0,
         139
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         0,
         0,
         0
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
