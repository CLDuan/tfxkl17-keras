{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Demo - Food Photo Classification\n",
    "\n",
    "In this demo, we are going to build a convolutional neural network using Keras, and train it to classify 4 types of foods - Ais Kacang, Ang Ku Kueh, Apam Balik, and Asam Laksa.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"datasets/train/AisKacang/001.jpg\" width=200 height=200 /></td>\n",
    "<td><img src=\"datasets/train/AngKuKueh/050.jpg\" width=200 height=200 /></td>\n",
    "<td><img src=\"datasets/train/ApamBalik/015.jpg\" width=200 height=200 /></td>\n",
    "<td><img src=\"datasets/train/AsamLaksa/012.jpg\" width=200 height=200 /></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The dataset photos are provided by [Jack](https://github.com/jackg0h/) from the FoodTag project. Try send a photo to the [Messenger bot](https://www.messenger.com/t/686970151472860) - it can recognize 100 types of Malaysian foods!\n",
    "\n",
    "References (citation mesti bagi):\n",
    "1. [Where I copied & pasted sample codes from](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n",
    "2. Goh Cheng Kee, Wong Chin Yee, John See (2017). FoodTag: Automatic Classification of Food Photos Using Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = 'datasets/train'\n",
    "test_dir  = 'datasets/test'\n",
    "nb_train_samples = 400\n",
    "nb_test_samples = 400\n",
    "img_width = 227\n",
    "img_height = 227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d11b5571-a344-4ecf-ac59-f0108365e052"
    }
   },
   "source": [
    "## Design the neural network\n",
    "\n",
    "Here, we build the convolutional neural network that will be used to classify the images.\n",
    "\n",
    "Read more:\n",
    "* [Keras - Convolution2D](https://keras.io/layers/convolutional/#convolution2d)\n",
    "* [Live demo of CNN in your browser (use Chrome)](http://cs231n.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "503aecb6-d4c3-4ac9-ac00-2138cd780668"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Most of these values like nb_filters, nb_rows, nb_columns are mostly trial and error.\n",
    "# There is no definite answer to what is the correct one, just keep trying.\n",
    "# input_shape[0] = 3 because of RGB channels\n",
    "model.add(Convolution2D(32, 5, 5, input_shape=(3, img_width, img_height)))\n",
    "\n",
    "# ReLU ensures none of the output value from CONV layer falls below 0\n",
    "# To prevent vanishing gradient problem (failure to learn) from happening\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# For every 2x2 \"pixels\" in the output feature map, pick the highest value,\n",
    "# more like pick the most activated one.\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Repeat a few times before going into fully connected (dense) layers\n",
    "# You are free to comment them out or add more conv relu pool layers\n",
    "# See what it does to the accuracy\n",
    "model.add(Convolution2D(64, 5, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(128, 5, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(256, 5, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We can do as many convolutions and poolings as we like, but it's time to really classify\n",
    "# Again, you can do as many dense layers as you like, use different activations and dropout %\n",
    "# It's all trial and error, this demo code may not be the most optimal after all!\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final layer, 4 nodes representing each class, softmax activation makes sure every node sums up to 1\n",
    "# That means the output nodes are in % score for each food.\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# The architecture is there. But how to measure how close we are to perfection?\n",
    "# Measure the difference from prediction to ground truth - the loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "01c045eb-8830-46c3-afe5-272c45577372"
    }
   },
   "source": [
    "## Prepare image data generator\n",
    "\n",
    "The inputs for neural networks are tensors - think matrices of numbers. In case of images, the tensors can be rows and columns of pixels, with each pixel quantised into red, green, blue channels. Hence, the 3 layers of convolutional neural networks, along with its fixed widths and heights.\n",
    "\n",
    "Keras provides this nifty data generator function to generate tensors of images during training. There are plenty of options that can be passed into the generator to augment images in real time. However, we won't be touching those yet.\n",
    "\n",
    "By simply passing in the directory names into the generator, you saved yourself from:\n",
    "* Manually generating RGB matrices from images\n",
    "* Resizing them to reduce processing on neural network's end (full-size photos may not fit into your graphics card VRAM)\n",
    "* Reshuffle them in every epoch\n",
    "\n",
    "See:\n",
    "- [Keras Blog - Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "- [ImageDataGenerator documentation](https://keras.io/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_generator = datagen.flow_from_directory(train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "testing_generator = datagen.flow_from_directory(test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e55b130-fe2d-4866-bf11-4d6c7362e756"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Since there are 4 food types (classes) to classify, a neural network that doesn't work will return random results - and yield 0.25 accuracy. So if you network keeps yielding 0.25 `val_acc` after a few epochs, check your setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e29767b5-810b-4237-ac87-a7a86f34e0a8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 16s - loss: 1.4386 - acc: 0.2400 - val_loss: 1.3846 - val_acc: 0.2500\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 12s - loss: 1.3698 - acc: 0.3000 - val_loss: 1.3854 - val_acc: 0.2625\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 12s - loss: 1.3576 - acc: 0.3250 - val_loss: 1.3087 - val_acc: 0.4375\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 12s - loss: 1.2506 - acc: 0.4025 - val_loss: 1.3124 - val_acc: 0.3350\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 12s - loss: 1.2115 - acc: 0.4500 - val_loss: 1.3345 - val_acc: 0.3350\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 12s - loss: 1.2433 - acc: 0.4175 - val_loss: 1.2268 - val_acc: 0.4200\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 12s - loss: 1.1623 - acc: 0.4575 - val_loss: 1.1487 - val_acc: 0.4800\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 12s - loss: 1.0689 - acc: 0.5300 - val_loss: 1.0943 - val_acc: 0.5100\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 12s - loss: 1.0155 - acc: 0.5175 - val_loss: 1.1081 - val_acc: 0.4950\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 12s - loss: 1.0032 - acc: 0.5275 - val_loss: 1.0771 - val_acc: 0.5125\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 12s - loss: 0.8514 - acc: 0.6175 - val_loss: 1.1062 - val_acc: 0.5825\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 12s - loss: 0.7740 - acc: 0.6525 - val_loss: 1.1766 - val_acc: 0.5575\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 12s - loss: 0.7226 - acc: 0.7050 - val_loss: 1.3674 - val_acc: 0.5275\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 12s - loss: 0.6711 - acc: 0.7025 - val_loss: 1.4097 - val_acc: 0.4900\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 12s - loss: 0.6606 - acc: 0.7500 - val_loss: 1.3119 - val_acc: 0.5475\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 12s - loss: 0.6090 - acc: 0.7500 - val_loss: 1.1885 - val_acc: 0.5725\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 12s - loss: 0.4746 - acc: 0.7875 - val_loss: 1.4127 - val_acc: 0.5825\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 12s - loss: 0.4598 - acc: 0.8025 - val_loss: 1.8782 - val_acc: 0.5600\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 12s - loss: 0.5204 - acc: 0.8025 - val_loss: 1.4109 - val_acc: 0.5725\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 12s - loss: 0.4097 - acc: 0.8350 - val_loss: 1.7988 - val_acc: 0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f48d0b385c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        training_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=20,\n",
    "        validation_data=testing_generator,\n",
    "        nb_val_samples=nb_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try augment the image data for higher accuracy\n",
    "\n",
    "In previous image data generator, we simply ask it to stream from the photo folders, and pass the photo at a smaller size into the convnet. But there may be several things we can do to make the training better:\n",
    "\n",
    "* Randomly rotate the images left and right because people don't always take food photos upright?\n",
    "* Randomly flip the photo horizontally?\n",
    "\n",
    "Keras built-in ImageDataGenerator is able to augment these images at training time, this is extremely handy when you have little training data, but you want your model to be able to generalise for wider range of applications - to look at different food photos better.\n",
    "\n",
    "#### How to know if rotating and flipping images is working?\n",
    "Look at the increase of `val_acc` and drop of `val_loss` compared to the log above. That means the neural network is able to generalise for the photos it has never looked at before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 12s - loss: 1.0583 - acc: 0.6075 - val_loss: 0.9763 - val_acc: 0.5950\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 12s - loss: 0.8808 - acc: 0.6550 - val_loss: 0.9268 - val_acc: 0.6650\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 12s - loss: 0.7738 - acc: 0.6975 - val_loss: 0.8121 - val_acc: 0.6800\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 12s - loss: 0.6883 - acc: 0.7275 - val_loss: 0.8977 - val_acc: 0.6825\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 12s - loss: 0.6769 - acc: 0.7200 - val_loss: 0.7337 - val_acc: 0.7375\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 12s - loss: 0.7044 - acc: 0.7350 - val_loss: 0.8874 - val_acc: 0.6725\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 12s - loss: 0.6677 - acc: 0.7625 - val_loss: 0.9357 - val_acc: 0.6600\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 12s - loss: 0.6262 - acc: 0.7700 - val_loss: 0.9014 - val_acc: 0.7150\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 12s - loss: 0.6059 - acc: 0.7675 - val_loss: 0.8453 - val_acc: 0.6925\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 12s - loss: 0.4962 - acc: 0.8225 - val_loss: 0.7683 - val_acc: 0.7425\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 12s - loss: 0.4473 - acc: 0.8350 - val_loss: 1.5801 - val_acc: 0.6575\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 12s - loss: 0.5833 - acc: 0.8025 - val_loss: 0.7677 - val_acc: 0.7500\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 12s - loss: 0.5505 - acc: 0.8125 - val_loss: 0.7871 - val_acc: 0.7525\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 12s - loss: 0.5055 - acc: 0.7875 - val_loss: 0.7806 - val_acc: 0.7500\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 12s - loss: 0.4480 - acc: 0.8425 - val_loss: 0.7066 - val_acc: 0.7625\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 12s - loss: 0.4233 - acc: 0.8375 - val_loss: 0.6873 - val_acc: 0.7825\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 12s - loss: 0.3417 - acc: 0.8600 - val_loss: 0.6622 - val_acc: 0.7825\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 12s - loss: 0.3622 - acc: 0.8675 - val_loss: 0.6602 - val_acc: 0.7825\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 12s - loss: 0.3461 - acc: 0.8750 - val_loss: 0.6975 - val_acc: 0.7450\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 12s - loss: 0.3475 - acc: 0.8725 - val_loss: 0.7475 - val_acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f48c41c1908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "training_generator = datagen.flow_from_directory(train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "testing_generator = datagen.flow_from_directory(test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "model.fit_generator(\n",
    "        training_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=20,\n",
    "        validation_data=testing_generator,\n",
    "        nb_val_samples=nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [p3]",
   "language": "python",
   "name": "Python [p3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "8eaa2e2c-fa56-4e39-8dcc-78c13c7628e1": {
     "id": "8eaa2e2c-fa56-4e39-8dcc-78c13c7628e1",
     "layout": "manual",
     "prev": null,
     "regions": {
      "55aee090-d18f-4028-a446-f6d4b8c3e269": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "7bc7d92f-c4ef-4997-bc94-27a1d7774054",
        "part": "source"
       },
       "id": "55aee090-d18f-4028-a446-f6d4b8c3e269"
      },
      "6110c4ae-a98a-4d57-a30b-f68f73c61500": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0.4969429347826087,
        "y": 0.01358695652173913
       },
       "content": {
        "cell": "7bc7d92f-c4ef-4997-bc94-27a1d7774054",
        "part": "outputs"
       },
       "id": "6110c4ae-a98a-4d57-a30b-f68f73c61500"
      }
     }
    }
   },
   "themes": {
    "default": "6b984cc6-d473-4bf7-8a5b-52a343aecfc6",
    "theme": {
     "6b984cc6-d473-4bf7-8a5b-52a343aecfc6": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "6b984cc6-d473-4bf7-8a5b-52a343aecfc6",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         0,
         0,
         0
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         0,
         0,
         139
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         0,
         0,
         0
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
